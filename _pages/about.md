---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Wenhua Cheng is a Software Engineer at Intel, with a strong background in large language model (LLM) quantization, model compression, and computer vision. Previously, he worked at Alibaba Cloud as a software engineer and at Intel Labs as a researcher. Wenhua holds a Masterâ€™s degree from Zhejiang University and a Bachelorâ€™s degree from Nanjing University of Science and Technology.

Wenhua's expertise spans two main domains:

1. **LLM Compression**: As the first author, he has contributed to methods such as **SignRound** and **SignRoundV2**, signed gradient descent-based rounding optimizations for LLM quantization, and **TEQ (Trainable Equivalent Transformation)**.

2. **Computer Vision**:  As a co-first author, he won **two championships** in the DawnBench competition, outperforming teams from Huawei, Google, and others. As the first author, he also ranked **4th and 6th** in two tracks of the **2017 ICDAR Scene Text Detection Competition**.

Wenhua has filed **21 patents**, 11 of which have been granted. Over the past four years, he has contributed to **300+ merged PRs**.
**He enjoys remote work. If you have such an opportunity, feel free to drop an email.**

[//]: # (# ğŸ”¥ News)

[//]: # (- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. )

[//]: # (- *2022.02*: &nbsp;ğŸ‰ğŸ‰ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. )


# ğŸ“ Publications 


<div class='paper-box'><div class='paper-box-image'><div><img src='images/arv2.png' alt="sym" width="100%"></div></div>

<div class='paper-box-text' markdown="1">


[2025] SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs [arxiv](https://arxiv.org/abs/2512.04746) <a href="https://github.com/intel/auto-round" target="_blank"><img src="https://img.shields.io/github/stars/intel/auto-round?style=social" alt="GitHub Stars"></a>

**Wenhua Cheng**, Weiwei Zhang, Heng Guo, Haihao Shen


</div>

</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/arv1.png' alt="sym" width="100%"></div></div>

<div class='paper-box-text' markdown="1">


[2023] Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs [EMNLP](https://aclanthology.org/2024.findings-emnlp.662/) <a href="https://github.com/intel/auto-round" target="_blank"><img src="https://img.shields.io/github/stars/intel/auto-round?style=social" alt="GitHub Stars"></a>

**Wenhua Cheng**, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Lv Kaokao, Yi Liu


</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><img src='images/teq.png' alt="sym" width="100%"></div></div>

<div class='paper-box-text' markdown="1">


[2023] TEQ: Trainable Equivalent Transformation for Quantization of LLMs [arxiv](https://arxiv.org/abs/2310.10944) <a href="https://github.com/intel/auto-round" target="_blank"><img src="https://img.shields.io/github/stars/intel/neural-compressor?style=social" alt="GitHub Stars"></a>

**Wenhua Cheng**, Yiyang Cai, Kaokao Lv, Haihao Shen


</div>

</div>

# ğŸ– Honors and Awards

- *2020* Ranked 1st in DawnBench competition (Training Time track)
- *2020* Ranked 1st in DawnBench competition (Training Cost track)
- *2017* Ranked 4th in DawnBench competition (Scene text detection track)
- *2017* Ranked 6th in DawnBench competition (COCO text detection track)


# ğŸ“– Educations

- 2011.09 â€“ 2014.06, Zhejiang University â€“ Control Science and Engineering

- 2005.09 â€“ 2009.06, Nanjing University of Science and Technology â€“ Automation

[//]: # ()
[//]: # (# ğŸ’¬ Invited Talks)

[//]: # (- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. )

[//]: # (- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]]&#40;https://github.com/&#41;)

[//]: # ()
[//]: # (# ğŸ’» Internships)

[//]: # (- *2019.05 - 2020.02*, [Lorem]&#40;https://github.com/&#41;, China.)